{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/random_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv('{}/tracks.csv'.format(data_dir), index_col=0)\n",
    "artists = pd.read_csv('{}/artists.csv'.format(data_dir), index_col=0)\n",
    "albums = pd.read_csv('{}/albums.csv'.format(data_dir), index_col=0)\n",
    "transactions = pd.read_csv('{}/transactions.csv'.format(data_dir), index_col=0)\n",
    "playlists = pd.read_csv('{}/playlists.csv'.format(data_dir), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К каждой транзакции добавляем признаки популярность(popular), артист(artistid), альбом(albumid) трека, имя плейлиста(name), num_followers плейлиста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_dataset(transactions, tracks, playlists):\n",
    "    full_dataset = pd.merge(transactions, tracks, how='left', on='trackid')\n",
    "    full_dataset = pd.merge(full_dataset, playlists, how='left', on='pid')\n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рейтинг определяем единичкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>trackid</th>\n",
       "      <th>popular</th>\n",
       "      <th>artistid</th>\n",
       "      <th>albumid</th>\n",
       "      <th>name</th>\n",
       "      <th>num_followers</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>822032</td>\n",
       "      <td>2283</td>\n",
       "      <td>3384</td>\n",
       "      <td>1151</td>\n",
       "      <td>1618</td>\n",
       "      <td>party time</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>822032</td>\n",
       "      <td>2288</td>\n",
       "      <td>4339</td>\n",
       "      <td>1152</td>\n",
       "      <td>1622</td>\n",
       "      <td>party time</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>822032</td>\n",
       "      <td>907</td>\n",
       "      <td>4992</td>\n",
       "      <td>500</td>\n",
       "      <td>687</td>\n",
       "      <td>party time</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>822032</td>\n",
       "      <td>896</td>\n",
       "      <td>5309</td>\n",
       "      <td>495</td>\n",
       "      <td>677</td>\n",
       "      <td>party time</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>822032</td>\n",
       "      <td>901</td>\n",
       "      <td>3481</td>\n",
       "      <td>495</td>\n",
       "      <td>681</td>\n",
       "      <td>party time</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pid  trackid  popular  artistid  albumid        name  num_followers  \\\n",
       "0  822032     2283     3384      1151     1618  party time              1   \n",
       "1  822032     2288     4339      1152     1622  party time              1   \n",
       "2  822032      907     4992       500      687  party time              1   \n",
       "3  822032      896     5309       495      677  party time              1   \n",
       "4  822032      901     3481       495      681  party time              1   \n",
       "\n",
       "   rating  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset = get_full_dataset(transactions, tracks, playlists)\n",
    "full_dataset['rating'] = 1\n",
    "full_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72955, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем основные модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polara.recommender.models import RecommenderModel\n",
    "from polara.recommender.data import RecommenderData\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import sparse\n",
    "from polara.recommender.coldstart.data import ItemColdStartData\n",
    "from polara.recommender.models import SVDModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описания модели\n",
    "\n",
    "- $\\textbf{R}$ - матрица playlist-track\n",
    "- $\\textbf{X}$ - матрица playlist-feuture\n",
    "- $\\textbf{A}$ - матрица близости плейлистов\n",
    "- $\\textbf{D}$ - диагональная матрица суммы строк $\\textbf{A}$\n",
    "- $\\textbf{L} = D - A$ - лапласиан\n",
    "\n",
    "$$J = \\frac{1}{2}[\\alpha||\\textbf{R} - \\textbf{UV}||_F^2 + (1 - \\alpha)||\\textbf{X} - \\textbf{UH}||_F^2 + \\beta Tr(\\textbf{U}^T\\textbf{LU}) + \\lambda(||\\textbf{U}||_F^2 + ||\\textbf{V}||_F^2 + ||\\textbf{H}||_F^2]$$\n",
    "\n",
    "$$arg \\min_{\\textbf{U}, \\textbf{V}, \\textbf{H}} J$$\n",
    "$$s.t. U \\geq 0, V \\geq 0, H \\geq 0$$\n",
    "\n",
    "Обновление $\\textbf{U}, \\textbf{V} , \\textbf{H}$:\n",
    "\n",
    "- $$U = U * \\frac{\\alpha\\textbf{R}\\textbf{V}^T + (1 -\\alpha)\\textbf{X}\\textbf{H}^T + \\beta\\textbf{AU}}{\\alpha\\textbf{UV}\\textbf{V}^T + (1 - \\alpha)\\textbf{UH}\\textbf{H}^T + \\beta\\textbf{DU} + \\lambda\\textbf{U}}$$\n",
    "- $$V = V * \\frac{\\alpha\\textbf{U}^T\\textbf{R}}{\\alpha\\textbf{U}^T\\textbf{UV} + \\lambda\\textbf{V}}$$\n",
    "- $$H = H * \\frac{(1 - \\alpha)\\textbf{U}^T\\textbf{X}}{(1 - \\alpha)\\textbf{U}^T\\textbf{UH} + \\lambda\\textbf{H}}$$\n",
    "\n",
    "$*$ - поэлементное умножение\n",
    "$\\frac{\\dots}{\\dots}$ - поэлементное деление"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс LocalCollectiveEmbeddings\n",
    "\n",
    "- get_train_content - получить признаки транзакций, content_data - df с признаками\n",
    "- get_content_shape - возвращает количество столбцов для каждого признака\n",
    "- get_training_content_matrix - возвращает матрицу features $n \\times v$ где n - кол-во плейлистов, v - количество признаков\n",
    "- get_test_content_matrix - аналогично для тестовой выборки\n",
    "- construct_closeness_matrix - строит матрицу близости плейлистов для по признакому описанию\n",
    "- update_factors - обновляет матрицы U, V, H по правилам выше\n",
    "- build - возвращает матрицу факторов плейлистов, треков, признаков\n",
    "    - аргументы df с признаками, параметры модели, maxiter, verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalCollectiveEmbeddings(RecommenderModel):\n",
    "    import scipy.sparse\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(LocalCollectiveEmbeddings, self).__init__(*args, **kwargs)\n",
    "        self.method = 'LCE'\n",
    "    \n",
    "    def get_train_content(self, content_data):\n",
    "        self.train_content = content_data\n",
    "    \n",
    "    def get_content_shape(self):\n",
    "        self.content_shape = {}\n",
    "        for col in self.train_content.columns:\n",
    "            self.content_shape[col] = self.train_content[col].max() + 1\n",
    "\n",
    "    def get_training_content_matrix(self):\n",
    "        self.get_content_shape()\n",
    "        idx_userid = self.data.training[self.data.fields[0]].values\n",
    "        val = np.ones(self.data.training.shape[0])\n",
    "        \n",
    "        i = 0\n",
    "        features = []\n",
    "        \n",
    "        for col in self.train_content.columns:\n",
    "            idx_feature = self.train_content[col].values\n",
    "            shp = (idx_userid.max() + 1, \n",
    "                   self.content_shape[col])\n",
    "        \n",
    "            features_new = sparse.csr_matrix((val, (idx_userid, idx_feature)), \n",
    "                                             shape=shp)\n",
    "            \n",
    "            if i == 0:\n",
    "                features = features_new\n",
    "            else:\n",
    "                features = sparse.hstack((features, features_new))\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def get_test_content_matrix(self):\n",
    "        idx_userid = self.data.test.testset[self.data.fields[0]].values\n",
    "        val = np.ones(self.data.test.testset.shape[0])\n",
    "        \n",
    "        i = 0\n",
    "        features = []\n",
    "        \n",
    "        for col in self.train_content.columns:\n",
    "            idx_feature = self.data.test.testset[col].values\n",
    "            shp = (idx_userid.max() + 1, \n",
    "                   self.content_shape[col])\n",
    "        \n",
    "            features_new = sparse.csr_matrix((val, (idx_userid, idx_feature)), \n",
    "                                             shape=shp)\n",
    "            \n",
    "            if i == 0:\n",
    "                features = features_new\n",
    "            else:\n",
    "                features = sparse.hstack((features, features_new))\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        \n",
    "        return features\n",
    "        \n",
    "        \n",
    "    def construct_closeness_matrix(self, X, rank, binary=False):\n",
    "        print ('Construct closeness matrix...')\n",
    "        nbrs = NearestNeighbors(n_neighbors=1 + rank).fit(X)\n",
    "        if binary:\n",
    "            closeness_matrix = nbrs.kneighbors_graph(X)\n",
    "        else:\n",
    "            closeness_matrix = nbrs.kneighbors_graph(X, mode='distance')\n",
    "        print ('Done.')\n",
    "            \n",
    "        return closeness_matrix\n",
    "    \n",
    "    def get_constant(self, R, X):\n",
    "        trRtR = tr(R, R)\n",
    "        trXtX = tr(X, X)\n",
    "        return trRtR, trXtX\n",
    "    \n",
    "    def update_factors(self, R, X, U, V, H, A, D, \n",
    "                       alpha, beta, lamb):\n",
    "        \n",
    "        gamma = 1. - alpha\n",
    "        \n",
    "        UtU = U.T.dot(U)\n",
    "        UtR = U.T.dot(R)\n",
    "        UtX = U.T.dot(X)\n",
    "        UtUV = UtU.dot(V)\n",
    "        UtUH = UtU.dot(H)\n",
    "        DU = D.dot(U)\n",
    "        AU = A.dot(U)\n",
    "        \n",
    "        #update V\n",
    "        V_1 = np.divide((alpha * UtR), \n",
    "                        (alpha * UtUV + lamb * V).maximum(1e-10))\n",
    "        V = V.multiply(V_1)\n",
    "            \n",
    "        #update H\n",
    "        H_1 = np.divide(\n",
    "            (gamma * UtX), (gamma * UtUH + lamb * H).maximum(1e-10))\n",
    "        H = H.multiply(H_1)\n",
    "            \n",
    "        # update U\n",
    "        U_t1 = alpha * R.dot(V.T) + gamma * X.dot(H.T) + beta * AU\n",
    "        U_t2 = alpha * U.dot(V.dot(V.T)) + gamma * \\\n",
    "        U.dot(H.dot(H.T)) + beta * DU + lamb * U\n",
    "            \n",
    "        U_t3 = np.divide(U_t1, (U_t2).maximum(1e-10))\n",
    "        U = U.multiply(U_t3)\n",
    "        \n",
    "        #calculate oblective function without constant\n",
    "        \n",
    "        tr1 = alpha * ((-2.) * tr(V, UtR) + tr(V, UtUV))\n",
    "        tr2 = gamma * ((-2.) * tr(H, UtX) + tr(H, UtUH))\n",
    "        tr3 = beta * (tr(U, DU) - tr(U, AU))\n",
    "        tr4 = lamb * (UtU.diagonal().sum() + tr(V, V) + tr(H, H))\n",
    "\n",
    "        Obj = tr1 + tr2 + tr3 + tr4\n",
    "        \n",
    "        \n",
    "        return U, V, H, Obj\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build(self, content_data, \n",
    "              rank=10, alpha=0.1, beta=0.005, lamb=0.0001, \n",
    "              epsilon=0.01, seed=0,maxiter=150, verbose=True):\n",
    "        \n",
    "        self.get_train_content(content_data)\n",
    "        \n",
    "        R = self.get_training_matrix(dtype='float64')\n",
    "        X = self.get_training_content_matrix()\n",
    "        A = self.construct_closeness_matrix(X, rank, binary=True).tocsr()\n",
    "        \n",
    "        num_users = R.shape[0]\n",
    "        num_items = R.shape[1]\n",
    "        num_features = X.shape[1]\n",
    "        \n",
    "        U = np.abs(sparse.rand(num_users, rank, 0.99, 'csr', dtype=R.dtype, random_state=seed))\n",
    "        V = np.abs(sparse.rand(rank, num_items, 0.99, 'csr', dtype=R.dtype, random_state=seed))\n",
    "        H = np.abs(sparse.rand(rank, num_features, 0.99, 'csr', dtype=R.dtype, random_state=seed))\n",
    "        \n",
    "        \n",
    "        #auxiliary constant   \n",
    "        D = sparse.dia_matrix((A.sum(axis=0), 0), A.shape)\n",
    "        trRtR, trXtX = self.get_constant(R, X)\n",
    "\n",
    "        itNum = 1\n",
    "        delta = 2.0 * epsilon\n",
    "\n",
    "        ObjHist = []\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            U, V, H, Obj = self.update_factors(R, X, U, V, H, A, D, \n",
    "                                               alpha, beta, lamb)\n",
    "            Obj += alpha*trRtR + (1. - alpha)*trXtX\n",
    "            Obj = Obj / (num_users * num_features * num_items * rank)\n",
    "            ObjHist.append(Obj)\n",
    "            \n",
    "            if itNum > 1:\n",
    "                delta = abs(ObjHist[-1] - ObjHist[-2])\n",
    "                if verbose:\n",
    "                    print (\"Iteration: \", itNum, \"Objective: \", Obj, \"Delta: \", delta)\n",
    "                if itNum > maxiter or delta < epsilon:\n",
    "                    break\n",
    "\n",
    "            itNum += 1\n",
    "            \n",
    "        self.user_factors = U\n",
    "        self.feature_factors = H \n",
    "        self.item_factors = V\n",
    "        \n",
    "        \n",
    "    def slice_recommendations(self, test_data, shape, start, stop, test_users=None):\n",
    "        slice_data = self._slice_test_data(test_data, start, stop)\n",
    "        features = self.get_test_content_matrix()\n",
    "        Ut = np.linalg.lstsq(self.feature_factors.T.toarray(), \n",
    "                             features.T.toarray(), rcond=-1)[0]\n",
    "        R = Ut.T.dot(self.item_factors.toarray())\n",
    "        return R, slice_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_content(content_data, col, sort=True, inplace=True):\n",
    "    grouper = content_data.groupby(col, sort=sort).grouper\n",
    "    new_val = grouper.group_info[1]\n",
    "    old_val = grouper.levels[0]\n",
    "    val_transform = pd.DataFrame({'old': old_val, 'new': new_val})\n",
    "    new_data = grouper.group_info[0]\n",
    "\n",
    "    if inplace:\n",
    "        result = val_transform\n",
    "        content_data.loc[:, col] = new_data\n",
    "    else:\n",
    "        result = (new_data, val_transform)\n",
    "    return result\n",
    "        \n",
    "def reindex_content_columns(content_data, columns):\n",
    "    index_content = {}\n",
    "    for col in columns:\n",
    "        index_content[col] = reindex_content(content_data, col)\n",
    "    return index_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr(A, B):\n",
    "    x = A.multiply(B)\n",
    "    return (x.sum(axis=0)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_lce(data, n_splits=5, seed=1, test_size=0.05, \n",
    "                         topk=500, rank=10, maxiter=10000, epsilon=0.1, \n",
    "                         alpha=.9, lamb=0.001, beta=0.05):\n",
    "    \n",
    "    index_content = reindex_content_columns(data, ['artistid', 'albumid'])\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, random_state=seed)\n",
    "    i = 1\n",
    "    scores_lce = {'precision': [], 'recall': [], 'miss_rate': [], 'nDCG': []}\n",
    "    scores_svd = {'precision': [], 'recall': [], 'miss_rate': [], 'nDCG': []}\n",
    "    for users_ids, unseen_users_ids in kf.split(data['pid'].drop_duplicates()):\n",
    "        print('=========================Fold {}============================='.format(i))\n",
    "        i += 1\n",
    "        users = data['pid'].drop_duplicates().values[users_ids]\n",
    "        unseen_users = data['pid'].drop_duplicates().values[unseen_users_ids]\n",
    "        train = data.query('pid in @users')\n",
    "        test = data.query('pid in @unseen_users')\n",
    "        train_albums = train.albumid.unique()\n",
    "        train_artists = train.artistid.unique()\n",
    "        test = test.query('albumid in @train_albums')\n",
    "        test = test.query('artistid in @train_artists')\n",
    "        test_sampled = test.sample(frac=1-test_size, random_state=seed).sort_values('pid')\n",
    "        test_holdout = test[~test.index.isin(test_sampled.index)]\n",
    "        \n",
    "        data_model = RecommenderData(train,'pid', 'trackid', 'rating', seed=seed)\n",
    "        data_model.prepare_training_only()\n",
    "        lce = LocalCollectiveEmbeddings(data_model)\n",
    "        lce.build(train[['artistid', 'albumid']], verbose=False, \n",
    "                  rank=rank, maxiter=maxiter, \n",
    "                  alpha=alpha, epsilon=epsilon, beta=beta, lamb=lamb)\n",
    "        \n",
    "        svd = SVDModel(data_model)\n",
    "        svd.rank = rank\n",
    "        svd.build()\n",
    "        \n",
    "        data_model.set_test_data(testset=test_sampled, holdout=test_holdout, warm_start=True)\n",
    "        lce.topk = topk\n",
    "        lce.swith_positive = 1\n",
    "        \n",
    "        svd.topk = topk\n",
    "        svd.swith_positive = 1\n",
    "        \n",
    "        hits = lce.evaluate()\n",
    "        relevance = lce.evaluate('relevance')\n",
    "        ranking = lce.evaluate('ranking')\n",
    "        \n",
    "        scores_lce['precision'].append(relevance.precision)\n",
    "        scores_lce['recall'].append(relevance.recall)\n",
    "        scores_lce['miss_rate'].append(relevance.miss_rate)\n",
    "        scores_lce['nDCG'].append(ranking.nDCG)\n",
    "        \n",
    "        print('lce hits', hits)\n",
    "        \n",
    "        hits = svd.evaluate()\n",
    "        relevance = svd.evaluate('relevance')\n",
    "        ranking = svd.evaluate('ranking')        \n",
    "        \n",
    "        scores_svd['precision'].append(relevance.precision)\n",
    "        scores_svd['recall'].append(relevance.recall)\n",
    "        scores_svd['miss_rate'].append(relevance.miss_rate)\n",
    "        scores_svd['nDCG'].append(ranking.nDCG)\n",
    "        \n",
    "        print ('svd hits', hits)\n",
    "        \n",
    "    result_lce = pd.DataFrame(scores_lce)\n",
    "    result_svd = pd.DataFrame(scores_svd)\n",
    "    return result_lce, result_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ===========rank=5=============\n",
      "=========================Fold 1=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/sparse/compressed.py:538: SparseEfficiencyWarning: Taking maximum (minimum) with > 0 (< 0) number results to a dense matrix.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PureSVD training time: 0.02512047698837705s\n",
      "33 unique trackid's within 36 testset interactions were filtered. Reason: not in the training data.\n",
      "5 unique trackid's within 5 holdout interactions were filtered. Reason: not in the training data.\n",
      "171 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=142, false_positive=4738, true_negative=None, false_negative=1276)\n",
      "svd hits Hits(true_positive=158, false_positive=4722, true_negative=None, false_negative=1260)\n",
      "=========================Fold 2=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.027114024007460102s\n",
      "27 unique trackid's within 30 testset interactions were filtered. Reason: not in the training data.\n",
      "3 unique trackid's within 3 holdout interactions were filtered. Reason: not in the training data.\n",
      "1 pid's were filtered out from holdout. Reason: inconsistent with testset.\n",
      "152 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=111, false_positive=4939, true_negative=None, false_negative=1310)\n",
      "svd hits Hits(true_positive=151, false_positive=4899, true_negative=None, false_negative=1270)\n",
      "=========================Fold 3=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.01901916298083961s\n",
      "35 unique trackid's within 40 testset interactions were filtered. Reason: not in the training data.\n",
      "3 unique trackid's within 3 holdout interactions were filtered. Reason: not in the training data.\n",
      "149 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=116, false_positive=4984, true_negative=None, false_negative=1353)\n",
      "svd hits Hits(true_positive=140, false_positive=4960, true_negative=None, false_negative=1329)\n",
      "=========================Fold 4=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.02389306199620478s\n",
      "25 unique trackid's within 31 testset interactions were filtered. Reason: not in the training data.\n",
      "4 unique trackid's within 4 holdout interactions were filtered. Reason: not in the training data.\n",
      "1 pid's were filtered out from holdout. Reason: inconsistent with testset.\n",
      "170 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=113, false_positive=4747, true_negative=None, false_negative=1261)\n",
      "svd hits Hits(true_positive=127, false_positive=4733, true_negative=None, false_negative=1247)\n",
      "=========================Fold 5=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.019203192001441494s\n",
      "43 unique trackid's within 51 testset interactions were filtered. Reason: not in the training data.\n",
      "3 unique trackid's within 3 holdout interactions were filtered. Reason: not in the training data.\n",
      "1 pid's were filtered out from holdout. Reason: inconsistent with testset.\n",
      "148 pid's were filtered out from testset. Reason: inconsistent with holdout.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:20<01:21, 20.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lce hits Hits(true_positive=124, false_positive=4966, true_negative=None, false_negative=1317)\n",
      "svd hits Hits(true_positive=152, false_positive=4938, true_negative=None, false_negative=1289)\n",
      "             ===========rank=10=============\n",
      "=========================Fold 1=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.03371043998049572s\n",
      "33 unique trackid's within 36 testset interactions were filtered. Reason: not in the training data.\n",
      "5 unique trackid's within 5 holdout interactions were filtered. Reason: not in the training data.\n",
      "171 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=173, false_positive=4707, true_negative=None, false_negative=1245)\n",
      "svd hits Hits(true_positive=208, false_positive=4672, true_negative=None, false_negative=1210)\n",
      "=========================Fold 2=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.03372105199377984s\n",
      "27 unique trackid's within 30 testset interactions were filtered. Reason: not in the training data.\n",
      "3 unique trackid's within 3 holdout interactions were filtered. Reason: not in the training data.\n",
      "1 pid's were filtered out from holdout. Reason: inconsistent with testset.\n",
      "152 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=144, false_positive=4906, true_negative=None, false_negative=1277)\n",
      "svd hits Hits(true_positive=211, false_positive=4839, true_negative=None, false_negative=1210)\n",
      "=========================Fold 3=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.03729436299181543s\n",
      "35 unique trackid's within 40 testset interactions were filtered. Reason: not in the training data.\n",
      "3 unique trackid's within 3 holdout interactions were filtered. Reason: not in the training data.\n",
      "149 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=163, false_positive=4937, true_negative=None, false_negative=1306)\n",
      "svd hits Hits(true_positive=201, false_positive=4899, true_negative=None, false_negative=1268)\n",
      "=========================Fold 4=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.03373700601514429s\n",
      "25 unique trackid's within 31 testset interactions were filtered. Reason: not in the training data.\n",
      "4 unique trackid's within 4 holdout interactions were filtered. Reason: not in the training data.\n",
      "1 pid's were filtered out from holdout. Reason: inconsistent with testset.\n",
      "170 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=144, false_positive=4716, true_negative=None, false_negative=1230)\n",
      "svd hits Hits(true_positive=185, false_positive=4675, true_negative=None, false_negative=1189)\n",
      "=========================Fold 5=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.03395385999465361s\n",
      "43 unique trackid's within 51 testset interactions were filtered. Reason: not in the training data.\n",
      "3 unique trackid's within 3 holdout interactions were filtered. Reason: not in the training data.\n",
      "1 pid's were filtered out from holdout. Reason: inconsistent with testset.\n",
      "148 pid's were filtered out from testset. Reason: inconsistent with holdout.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:50<01:15, 25.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lce hits Hits(true_positive=146, false_positive=4944, true_negative=None, false_negative=1295)\n",
      "svd hits Hits(true_positive=199, false_positive=4891, true_negative=None, false_negative=1242)\n",
      "             ===========rank=15=============\n",
      "=========================Fold 1=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.16532025401829742s\n",
      "33 unique trackid's within 36 testset interactions were filtered. Reason: not in the training data.\n",
      "5 unique trackid's within 5 holdout interactions were filtered. Reason: not in the training data.\n",
      "171 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=204, false_positive=4676, true_negative=None, false_negative=1214)\n",
      "svd hits Hits(true_positive=254, false_positive=4626, true_negative=None, false_negative=1164)\n",
      "=========================Fold 2=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.051894608011934906s\n",
      "27 unique trackid's within 30 testset interactions were filtered. Reason: not in the training data.\n",
      "3 unique trackid's within 3 holdout interactions were filtered. Reason: not in the training data.\n",
      "1 pid's were filtered out from holdout. Reason: inconsistent with testset.\n",
      "152 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=169, false_positive=4881, true_negative=None, false_negative=1252)\n",
      "svd hits Hits(true_positive=224, false_positive=4826, true_negative=None, false_negative=1197)\n",
      "=========================Fold 3=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.04434400697937235s\n",
      "35 unique trackid's within 40 testset interactions were filtered. Reason: not in the training data.\n",
      "3 unique trackid's within 3 holdout interactions were filtered. Reason: not in the training data.\n",
      "149 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=175, false_positive=4925, true_negative=None, false_negative=1294)\n",
      "svd hits Hits(true_positive=233, false_positive=4867, true_negative=None, false_negative=1236)\n",
      "=========================Fold 4=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.04929377298685722s\n",
      "25 unique trackid's within 31 testset interactions were filtered. Reason: not in the training data.\n",
      "4 unique trackid's within 4 holdout interactions were filtered. Reason: not in the training data.\n",
      "1 pid's were filtered out from holdout. Reason: inconsistent with testset.\n",
      "170 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=150, false_positive=4710, true_negative=None, false_negative=1224)\n",
      "svd hits Hits(true_positive=215, false_positive=4645, true_negative=None, false_negative=1159)\n",
      "=========================Fold 5=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.049294333992293105s\n",
      "43 unique trackid's within 51 testset interactions were filtered. Reason: not in the training data.\n",
      "3 unique trackid's within 3 holdout interactions were filtered. Reason: not in the training data.\n",
      "1 pid's were filtered out from holdout. Reason: inconsistent with testset.\n",
      "148 pid's were filtered out from testset. Reason: inconsistent with holdout.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [01:27<00:58, 29.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lce hits Hits(true_positive=147, false_positive=4943, true_negative=None, false_negative=1294)\n",
      "svd hits Hits(true_positive=241, false_positive=4849, true_negative=None, false_negative=1200)\n",
      "             ===========rank=25=============\n",
      "=========================Fold 1=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.09632662899093702s\n",
      "33 unique trackid's within 36 testset interactions were filtered. Reason: not in the training data.\n",
      "5 unique trackid's within 5 holdout interactions were filtered. Reason: not in the training data.\n",
      "171 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=229, false_positive=4651, true_negative=None, false_negative=1189)\n",
      "svd hits Hits(true_positive=318, false_positive=4562, true_negative=None, false_negative=1100)\n",
      "=========================Fold 2=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.08050106398877688s\n",
      "27 unique trackid's within 30 testset interactions were filtered. Reason: not in the training data.\n",
      "3 unique trackid's within 3 holdout interactions were filtered. Reason: not in the training data.\n",
      "1 pid's were filtered out from holdout. Reason: inconsistent with testset.\n",
      "152 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=196, false_positive=4854, true_negative=None, false_negative=1225)\n",
      "svd hits Hits(true_positive=274, false_positive=4776, true_negative=None, false_negative=1147)\n",
      "=========================Fold 3=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.07843299201340415s\n",
      "35 unique trackid's within 40 testset interactions were filtered. Reason: not in the training data.\n",
      "3 unique trackid's within 3 holdout interactions were filtered. Reason: not in the training data.\n",
      "149 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=210, false_positive=4890, true_negative=None, false_negative=1259)\n",
      "svd hits Hits(true_positive=280, false_positive=4820, true_negative=None, false_negative=1189)\n",
      "=========================Fold 4=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.08695739600807428s\n",
      "25 unique trackid's within 31 testset interactions were filtered. Reason: not in the training data.\n",
      "4 unique trackid's within 4 holdout interactions were filtered. Reason: not in the training data.\n",
      "1 pid's were filtered out from holdout. Reason: inconsistent with testset.\n",
      "170 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=184, false_positive=4676, true_negative=None, false_negative=1190)\n",
      "svd hits Hits(true_positive=259, false_positive=4601, true_negative=None, false_negative=1115)\n",
      "=========================Fold 5=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.0788751510262955s\n",
      "43 unique trackid's within 51 testset interactions were filtered. Reason: not in the training data.\n",
      "3 unique trackid's within 3 holdout interactions were filtered. Reason: not in the training data.\n",
      "1 pid's were filtered out from holdout. Reason: inconsistent with testset.\n",
      "148 pid's were filtered out from testset. Reason: inconsistent with holdout.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [02:30<00:37, 37.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lce hits Hits(true_positive=213, false_positive=4877, true_negative=None, false_negative=1228)\n",
      "svd hits Hits(true_positive=285, false_positive=4805, true_negative=None, false_negative=1156)\n",
      "             ===========rank=50=============\n",
      "=========================Fold 1=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.24990174898994155s\n",
      "33 unique trackid's within 36 testset interactions were filtered. Reason: not in the training data.\n",
      "5 unique trackid's within 5 holdout interactions were filtered. Reason: not in the training data.\n",
      "171 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=251, false_positive=4629, true_negative=None, false_negative=1167)\n",
      "svd hits Hits(true_positive=346, false_positive=4534, true_negative=None, false_negative=1072)\n",
      "=========================Fold 2=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.20747292498708703s\n",
      "27 unique trackid's within 30 testset interactions were filtered. Reason: not in the training data.\n",
      "3 unique trackid's within 3 holdout interactions were filtered. Reason: not in the training data.\n",
      "1 pid's were filtered out from holdout. Reason: inconsistent with testset.\n",
      "152 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=223, false_positive=4827, true_negative=None, false_negative=1198)\n",
      "svd hits Hits(true_positive=292, false_positive=4758, true_negative=None, false_negative=1129)\n",
      "=========================Fold 3=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n",
      "PureSVD training time: 0.36138673100504093s\n",
      "35 unique trackid's within 40 testset interactions were filtered. Reason: not in the training data.\n",
      "3 unique trackid's within 3 holdout interactions were filtered. Reason: not in the training data.\n",
      "149 pid's were filtered out from testset. Reason: inconsistent with holdout.\n",
      "lce hits Hits(true_positive=230, false_positive=4870, true_negative=None, false_negative=1239)\n",
      "svd hits Hits(true_positive=320, false_positive=4780, true_negative=None, false_negative=1149)\n",
      "=========================Fold 4=============================\n",
      "Preparing data...\n",
      "Done.\n",
      "Construct closeness matrix...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "ranks = [5, 10, 15, 25, 50]\n",
    "recall_lce, recall_svd = [], []\n",
    "nDCG_lce, nDCG_svd = [], []\n",
    "precision_lce, precision_svd  = [], []\n",
    "miss_rate_lce, miss_rate_svd= [], []\n",
    "for rank in tqdm.tqdm(ranks):\n",
    "    print('             ===========rank={}============='.format(rank))\n",
    "    result_lce, result_svd = cross_validation_lce(full_dataset,n_splits=5, topk=10, test_size=0.1, \n",
    "                                                  rank=rank, alpha=0.9, epsilon=1e-11, \n",
    "                                                  lamb=1, beta=0.05)\n",
    "    recall_lce.append(result_lce.recall.mean())\n",
    "    nDCG_lce.append(result_lce.nDCG.mean())\n",
    "    precision_lce.append(result_lce.precision.mean())\n",
    "    miss_rate_lce.append(result_lce.miss_rate.mean())\n",
    "    recall_svd.append(result_svd.recall.mean())\n",
    "    nDCG_svd.append(result_svd.nDCG.mean())\n",
    "    precision_svd.append(result_svd.precision.mean())\n",
    "    miss_rate_svd.append(result_svd.miss_rate.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(ranks, recall_lce, 'orange', label='recall_lce')\n",
    "plt.plot(ranks, recall_svd, label='recall_svd')\n",
    "plt.ylabel('evalution')\n",
    "plt.xlabel('rank')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(ranks, nDCG_lce,label='nDCG_lce')\n",
    "plt.plot(ranks, nDCG_svd, label='nDCG_svd')\n",
    "plt.ylabel('evalution')\n",
    "plt.xlabel('rank')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
