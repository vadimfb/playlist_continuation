\documentclass[12pt,twoside]{article}


\usepackage{graphicx}
\usepackage{caption}
\usepackage{jmlda}

\begin{document}
\title
    {Автоматическое дополнение плейлистов в рекомендательной системе пользователей}
\author
    {Кислинский~В.\,Г., Фролов~Е.\,, Воронцов~К.\,В.} % основной список авторов, выводимый в оглавление
\email
    {kislinskiy.vg@phystech.edu; evgeny.frolov@skolkovotech.ru; vokov@forecsys.ru}
\thanks
    {Работа выполнена при финансовой поддержке РФФИ, проект \No\,00-00-00000.
     Научный руководитель:  Воронцов~К.\,В.
     Консультант:  Фролов~Е.}

\organization
    {Московский физико-технический институт}
\abstract
	{Работа посвящена исследованию метода совместной матричной факторизации в задаче top-N рекомендаций для автоматического продолжения плейлистов. Предлагается  модель матричной факторизации, учитывающий дополнительную информацию о плейлистах и треках. Данный метод будет иметь, не только преимущество алгоритмов коллаборативной фильтрации, которые способны выявить скрытые свойства пользователей и обьектов,  но также сможеть учитывать контекстную информацию, что поможет решить проблему холодного старта для обьектов. В данном методе будет введена дополнительная регуляризация, основанная на предположение, что если обьекты близки в пространстве признаков, то они близки в латентном факторном пространстве. Для анализа качества представленного алгоритма проводятся эксперименты на выборке  из миллиона плейлистов MPD. 

\bigskip
\textbf{Ключевые слова}: \emph {задача top-N рекомендаций, совместная матричная факторизация, алгоритм LCE, латентное факторное пространство, коллаборативная фильтрация}.

}

\maketitle

\section{1 Введение}

Большинство методов коллаборативной фильтрации имеют ряд недостатков, основным из которых является проблема холодного старта. Другой подход к задаче рекомендаций, основанный на дополнительный информации, не имеет этой проблемы. Поэтому комбинирования этих методов ~\cite{sutskever2014}. Поэтому выбор и поиск оптимальной структуры  нейронной сети также является вычислительно сложной процедурой, которая сильно влияет на итоговое качество модели. Использование переусложненных моделей с избыточным количеством неинформативных параметров также является препятствием для использования глубоких сетей на мобильных устройствах в режиме реального времени.

Существуют разные подходы к построению оптимальной сети. В работах~\cite{maclarin2015, luketina2015} предлагается использовать модель градиентного спуска для оптимизации сети. В ряде работ~\cite{molchanov2017, ullrich2016} используются байесовские методы~\cite{neal1995} оптимизации параметров нейронных сетей. 

Другим методом поиска оптимальной структуры является прореживание переусложненной модели~\cite{cun1990, louizos2017, graves2011}. В работе~\cite{cun1990} предлагатся удалять наименее релевантные параметры на основе значений первой и второй производных функции ошибки.

Данная работа посвящена прореживанию структуры сети. Предлагается удалять наименее \textit{релевантные} параметры модели~\cite{cun1990}. Метод предлагает построение исходной избыточной сложности нейросети с большим количеством избыточных параметров. Для определения релевантности параметров предлагается оптимизацировать параметры и гиперпараметры в единой процедуре. Для удаления параметров предлагается использовать метод Белсли.

Эксперимент метода проводится на выборке MNIST и синтетических данных. Результат сравнивается с моделью полученной при помощи алгоритма AdaNet~\cite{cortes2017}.

\section{2 Постановка задачи}

Задана выборка
$$\mathfrak{D} = \{\textbf{x}_i,y_i\},~ i =1...N, \eqno(2.1)$$
где~$\textbf{x}_i \in \mathbb{R}^{n}$,~$y_i \in \{1, \dots, Y\}$ , где~$Y$ --- число классов.

Рассмотрим модель~$f(\mathbf{x}, \mathbf{w}): \mathbb{R}^n \times \mathbb{R}^m \to \{1,\dots,Y\}$, где~$\textbf{w} \in \mathbb{R}^m$ --- пространство параметров модели.

$$f(\mathbf{x}, \mathbf{w}) = \text{softmax}\bigl( f_1(f_2(...(f_l(\mathbf{x}, \mathbf{w})\bigr), \eqno(2.2)$$
где~$f_i(\mathbf{x}, \mathbf{w}) =  \text{tanh}(\mathbf{w}\mathbf{x})$,~$l$ --- число слоев нейронной сети,~$i \in \{1\dots l\}$.


Параметр~$w_i$ модели~$f$  называется активным, если~$w_i \not = 0$. Множество индексов активных параметров обозначим~$\mathcal{A}$.

Задано множество параметров удовлетворяющих множеству активных параметров:

$$\mathbb{W_\mathcal{A}} = \{ \textbf{w} \in \mathbb{R}^m~|~w_i\not=0,~i \in \mathcal{A}  \}, \eqno(2.3)$$


Для модели~$f$ с множеством активных параметров~$\mathcal{A}$ и соответствующего ей вектора параметров~$\textbf{w} \in \mathbb{W_\mathcal{A}}$  определим логарифмическую функцию правдоподобия выборки~$\mathcal{L}_\mathfrak{D}(\mathfrak{D},\mathcal{A}, \textbf{w}):$
$$\mathcal{L}_\mathfrak{D}(\mathfrak{D}, \mathcal{A}, \textbf{w}) = \log p(\textbf{y|x},\mathcal{A},\textbf{w}) = \log p(\mathfrak{D}|\mathcal{A},\textbf{w}), \eqno(2.4)$$
где~$p(\textbf{y|x,w},\mathcal{A})$ --- апостериорная вероятность вектора~$\textbf{y}$ при заданных~$\textbf{x,w}, \mathcal{A}$.

Оптимальные~$\textbf{w},\mathcal{A}$ находятся из минимизации~$-\mathcal{L}_\mathcal{A}(\mathfrak{D},\mathcal{A},\textbf{w})$ --- логарифма правдоподобия модели:
$$\mathcal{L}_\mathcal{A}(\mathfrak{D},\mathcal{A},\textbf{w}) =\log p(\textbf{y}|\textbf{x},\mathcal{A},\textbf{w}) = \log \int_{w\in\mathbb{W_\mathcal{A}}} 
p(\textbf{y} | \mathcal{A}, \textbf{w}) p(\textbf{w} | \mathcal{A}) d \textbf{w}, \eqno(2.5)$$
где~$p(\textbf{w}|\mathcal{A})$ --- априорная вероятность вектора параметров в пространстве~$\mathbb{W_\mathcal{A}}$.

Рассмотрим вариационный подход для решения этой задачи. Пусть задано распределение~$q$, аппроксимирующее неизвестное апостериорное распределение~$p(\textbf{w}|\mathfrak{D},\mathcal{A})$:

$$q(\textbf{w})\sim \mathcal{N}(\textbf{m}, \textbf{A}^{-1}_\text{ps}), \eqno(2.6)$$
где~$\textbf{m}, \textbf{A}^{-1}_\text{ps}$ --- вектор средних и матрица ковариации.
$$p(\textbf{w} | \mathcal{A})\sim \mathcal{N}(\boldsymbol{\mu},\textbf{A}^{-1}_{\text{pr}}), \eqno(2.7)$$
где~$\boldsymbol{\mu},\textbf{A}^{-1}_{\text{pr}}$ --- вектор средних и матрица ковариации.


Приблизим интеграл (2.5) вариационной оценкой \cite{graves2011}:
$$\mathcal{L}(\mathfrak{D},\mathcal{A},\textbf{w}) = \mathcal{L}_\textbf{w}(\mathfrak{D}, \mathcal{A}, \textbf{w})+\mathcal{L}_{E}(\mathfrak{D},\mathcal{A}), \eqno(2.8)$$

Первое слагаемое формулы (2.8) это сложность модели, которое определяется расстоянием Кульбака-Лейблера:
$$\mathcal{L}_\textbf{w}(\mathfrak{D}, \mathcal{A}, \textbf{w}) = D_{KL}\bigl(q(\textbf{w})||p(\textbf{w}|\mathcal{A})\bigr), \eqno(2.9)$$

Второе слагаемое формулы (2.8) является матожиданием правдоподобия выборки~$\mathcal{L}_\mathfrak{D}(\mathfrak{D},\mathcal{A}, \textbf{w})$, которое интерпретируется как функция ошибки:
$$\mathcal{L}_{E}\mathfrak{D},\mathcal{A}) = \mathsf{E}_{\textbf{w}\sim q}\mathcal{L}_\mathfrak{D}(\textbf{y}, \mathfrak{D}, \mathcal{A}, \textbf{w}), \eqno(2.10)$$

Требуется найти параметры, доcтавляющие минимум суммарному функционалу потерь~$\mathcal{L}$:
$$\textbf{w} = \argmin_{\textbf{w} \in \mathbb{W_\mathcal{A}},~\mathcal{A}\in2^{m}} -\mathcal{L}(\mathfrak{D}, \mathcal{A}, \textbf{w}), \eqno(2.11)$$



\section{3 Базовый метод}

\subsection{3.1 Случайное удаление}
Метод случайного удаления заключается в том, что мы случайным образом удаляем некоторые нейроны из сети. 

Тоесть:
$$\xi \in U(\mathcal{A}), \eqno(3.1.1)$$
где~$\xi$ удаляемый параметр.

\subsection{3.2 Optimal Brain Damage}
Метод \cite{cun1990}, использует вторую производную целевой функции по параметрам для определения не релевантных параметров. Рассмотрим функцию потерь~$\mathcal{L}$ из (2.4) --- разложенную в ряд Тейлора в некоторой окрестности вектора параметров~$\textbf{w}$:
$$\delta \mathcal{L} = \sum_{i\in \mathcal{A}} g_i\delta u_i + \frac{1}{2}\sum_{i,j\in \mathcal{A}} h_{ij}\delta w_i\delta w_j + O(||\delta\textbf{w}||^3), \eqno(3.2.1)$$
где~$\delta w_i~$ --- компоненты вектора~$\delta\textbf{w}$,~$g_i$ --- компоненты вектора градиента~$\nabla \mathcal{L}$, а~$h_{ij}$ --- компоненты гесcиана~$\textbf{H}$:
$$g_i = \frac{\partial \mathcal{L}}{\partial w_i} \qquad h_{ij} = \frac{\partial^2\mathcal{L}}{\partial w_i \partial w_j}. \eqno(3.2.2)$$

Задача является вычислительно сложной в силу размерности матрицы \textbf{H}. Введем следующее предположение \cite{cun1990}, о том что удаление нескольких параметров приводит к такому же изменению функции потерь~$\mathcal{L}$, как и суммарное изменение при индивидуальном удалении:
$$\delta \mathcal{L} = \sum_{i\in \mathcal{A}}^N \delta \mathcal{L}_i, \eqno(3.2.3)$$
где~$N$ --- число удаляемых параметров,~$\mathcal{L}_i$ --- изменение функции потерь, при удалении одного параметра~$\textbf{w}_i$.

В силу данного предположения будем рассматривать только диагональные элементы матрицы \textbf{H}. После введенного предположения, (3.2.1) принимает вид:
$$\delta \mathcal{L} = \frac{1}{2} \sum_{i\in \mathcal{A}} h_{ii}\delta u_i^2, \eqno(3.2.4)$$

Релевантность параметров определяется следующим образом:
$$s_i = h_{ii}\frac{w_i^2}{2}. \eqno(3.2.5)$$

Получаем следующую задачу оптимизации:

$$\xi = \argmin_{j\in \mathcal{A}} s_{j}, \eqno(3.2.6)$$
где~$\xi$ --- наименее релевантный параметр.

\subsection{3.3 Удаление неинформативных параметров с помощью вариационного вывода}
Для удаления параметров в работе \cite{graves2011} предлагается удалить параметры, которые имеют наибольшую плотность апостериорной вероятности~$\rho$ в нуле.

Для гауссовского распределения в работе \cite{graves2011} была предложена следующая задача оптимизации:
$$\xi = \argmin_{i\in \mathcal{A}} \left|\frac{\mu_i}{\sigma_i}\right|, \eqno(3.3.1)$$
где~$\xi$ --- наименее релевантный параметр.

\section{4 Метод Белсли}
Помимо вышеописанных методов, предлагается метод основанный на модификации метода Белсли.

Пусть $\textbf{w}$ --- вектор параметров доставляющий минимум функционалу потерь $\mathcal{L}$ на  множестве $\mathbb{W_\mathcal{A}}$, а $\textbf{A}^{-1}_\text{ps}$ соответствующая ему ковариационная матрица.

Выполним сингулярное разложение матрицы $\textbf{A}^{-1}_\text{ps}$:

$$\textbf{A}^{-1}_\text{ps} = \textbf{L}\textbf{L}^\mathrm{T} = \textbf{U}{\bf\Lambda}\textbf{V}^\mathrm{T}\textbf{V}{\bf\Lambda}^\mathrm{T}\textbf{U}^\mathrm{T} = \textbf{U}{\bf\Lambda}^2\textbf{U}^\mathrm{T}$$

Долевой коэффициент $q_{ij}$ определим как вклад $j$-го признака в дисперсию $i$-го элемента вектора параметра параметров $\textbf{w}$. 

$$q_{ij} = \frac{u^2_{ij}\lambda_{jj}}{D(w_i)}, \eqno(4.1)$$
где $D(w_i)$ --- дисперсия параметра $w_i$.

Используя метод (3.3.1) находим наименее релевантный параметр из набора параметров $\mathcal{A}$ --- обозначим его $\xi$. Затем находим максимальные долевые коэффициента, соответствующие данному параметру $w_\xi$:

$$\zeta = \argmax_{j\in \mathcal{A}}{q_{\xi j}}. \eqno(4.2)$$

Параметры  $\xi$ и $\zeta$ определим как наименее релевантные параметры нейросети.


\section{5 Базовый вычислительный эксперимент}

	В базовом эксперименте сравнивается качество и скорость сходимости трех моделей --- полной нейронной сети, сети с произвольно удаленными параметрами и сети полученной при помощи Optimal Brain Damage.
	
	В качестве исходных данных была использована выборка из 178  результатов химического анализа вин \footnote{http://archive.ics.uci.edu/ml/datasets/Wine}, по которым нужно было распределить вино по классам.

\begin{figure}[!htb]
	\center{
		\subfloat[]{\includegraphics[width=0.45\textwidth]{results/train.pdf} }
		\subfloat[]{\includegraphics[width=0.45\textwidth]{results/test.pdf}} 
	}
	\caption{
		Зависимости значения функции потерь от процента удаленных параметров.
	}
	\label{results}
\end{figure}

	В результате эксперимента были получены следующие результаты, показаные на рисунке \ref{results}. На графике \ref{results}a изображена зависимость для обучающей выборки. На графике \ref{results}b --- зависимость среднего значения функции потерь для тестовой выборки.
	
	Из графиков видно, что на тестовой выборке при небольшом количестве удаляемых параметров, метод OBD дает лучше результат, чет произвольное удаление параметров

\bibliography{Grabovoy2018OptimalBrainDamage}
\bibliographystyle{unsrt}

\end{document}
